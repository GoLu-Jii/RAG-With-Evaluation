from dotenv import load_dotenv
import os

load_dotenv()

from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

from langchain_groq import ChatGroq
from ragas.llms import LangchainLLMWrapper

GROQ_API_KEY = os.getenv("GROQ_API_KEY")

evaluator_llm = LangchainLLMWrapper(
    ChatGroq(
        groq_api_key=GROQ_API_KEY,
        model_name="mixtral-8x7b-32768",
        temperature=0
    )
)


data = {
    "question": [
        "Which French presidents' end-of-the-year addresses were used as the basis for comparison in this study?",
        "What are the main grammatical categories that ChatGPT tends to overuse compared to the French presidents?",
        "Which specific verbs and pronouns were identified as being overused by ChatGPT?",
        "What does the paper define as 'hallucinations' in the context of generated texts?",
        "How accurate are black-box classifiers (like RoBERTa) at detecting texts generated by GPT-3?",
        "Under what conditions does the accuracy of automatic machine-generated text detectors drop significantly?",
        "How do the lengths of ChatGPT-generated texts generally compare to the natural texts used as models?",
        "Why did the researchers apply orthographic standardization to both the natural and generated texts?",
        "Which specific verb tenses does ChatGPT avoid using, likely due to their complexity?",
        "Why does ChatGPT show a strong preference for common nouns, adjectives, possessives, and prepositions?",
        "What are the three most frequently used verbs in natural French texts?",
        "How much did ChatGPT's usage of the pronoun 'we' (nous) increase compared to the presidents' usage?",
        "Why is the negative construction ('ne+verb+pas') almost halved in the texts generated by ChatGPT?",
        "Which adverb ranks 37th among the presidents but jumps to the third most popular adverb for ChatGPT?",
        "What specific noun saw a 696% increase in usage by ChatGPT, and why?",
        "How did ChatGPT alter the presidents' traditional greeting to their 'fellow citizens'?",
        "In the corpus of presidential addresses, what is the most frequent sentence length (mode) compared to the ChatGPT texts?",
        "How does the distribution of sentence lengths in ChatGPT texts compare to natural texts?",
        "What mathematical inequality characterizes sentence lengths in natural texts?",
        "How does ChatGPT seem to treat punctuation when generating sentences?",
        "Is intertextual distance effective at identifying ChatGPT-generated texts in this study?",
        "Why are conventional n-gram-based plagiarism detectors likely to fail against ChatGPT?",
        "Based on the hierarchical classification, which president's style is most distinctly out of step with his successors?",
        "What was the quality index score for the tree classification used to analyze the presidents' speeches?",
        "What is the primary target text genre for ChatGPT based on its training?"
    ],
    "answer": [
        "Chirac, Sarkozy, Hollande, and Macron",
        "I don't know.",
        "ChatGPT tends to overuse \"to must\" (devoir), \"to continue\", and the lemma \"we\" (nous)",
        "In the context of generated texts, 'hallucinations' are defined as incoherence and inaccuracies, or incorrect information, in the output ",
        "I don't know.",
        "The accuracy of automatic machine-generated text detectors drops significantly when faced with a new and unknown domain or when substituting tokens by misspelled words, with accuracy rates varying from 28% to 60%",
        "The lengths of the texts generated by ChatGPT fluctuate considerably",
        "The researchers applied orthographic standardization to ensure that any differences observed between NTs and GPTs do not stem from fluctuations in word spelling",
        "The imperfect tense and the past simple tense ",
        "ChatGPT shows a clear preference for common nouns, adjectives, possessives, and prepositions, probably because their associations with other words are more regular",
        "“être” (to be), “avoir” (to have), and “faire” (to do)",
        "ChatGPT's usage of the pronoun 'we' increased by 51% compared to the presidents' usage.",
        "The negative construction (\"ne+verb+pas\") is almost halved in ChatGPT-generated texts because it is employed exclusively with a verb whose subject is often a pronoun like \"ce n'est pas\" (it is not) ",
        "\"également\" (also) ",
        "I don't know.",
        "I don't know.",
        "In the corpus of presidential addresses, the most frequent sentence length (mode) is 12 words, whereas in the GPTs (ChatGPT texts), it is 19 words, which is 46% longer.",
        "The distribution of sentence lengths in ChatGPT texts is dissimilar to natural texts, with ChatGPT's distribution being more concentrated in the middle and having fewer very short or very long sentences",
        "mode < median < mean",
        "ChatGPT treats punctuation like words.",
        "No, according to [480583a9-3321-478d-ba8e-6ea304e69162_p20_c1], intertextual distance is no longer able to identify texts generated by ChatGPT.",
        "Conventional plagiarism detection systems are likely to be ineffective because they analyze n-grams, whereas GPT rearranges the vocabulary of the model in the generated text ",
        "I don't know.",
        "The quality of the whole tree is greater than 96%, and all the paths on the tree have an index greater than 90%.",
        "The primary target text genre for ChatGPT is a short response within a question/answering system, taking into account previous interactions "
    ],
    "contexts": [
        ["3 Corpus Overview To compare the written style of recent French presidents with speeches generated by machine, ChatGPT (chat.openai.com) was asked to generate the end-of-the-year address of four presidents, namely Chirac, Sarkozy, Hollande, and Macron.  For each leader and each year, we asked ChatGPT to generate an end-of-the-year address after submitting it the corresponding natural text (NT) as a model", "3 Corpus Overview To compare the written style of recent French presidents with speeches generated by machine, ChatGPT (chat.openai.com) was asked to generate the end-of-the-year address of four presidents, namely Chirac, Sarkozy, Hollande, and Macron.  For each leader and each year, we asked ChatGPT to generate an end-of-the-year address after submitting it the corresponding natural text (NT) as a model", "The main focus of this study is to analyze the written style of one LLM called ChatGPT by comparing its generated messages with those of the recent French presidents.  To achieve this, we compare end-of-the-year addresses written by Chirac, Sarkozy, Hollande, and Macron with those automatically produced by ChatGPT.  We found that ChatGPT tends to overuse nouns, possessive determiners, and numbers"],
        ["The characteristics of ChatGPT's vocabulary (see Section 4) suggest that ChatGPT does not really know French grammar and all the vocabulary, but that it works with a wide variety of tokens (graphic forms) and seems to have some difficulty with certain homographs.   To have an overview of our corpus, Table 1 provides the president names and Nnt, the number of words in each presidential message, followed by Ngpt, the number of words of the corresponding generated text", "The characteristics of ChatGPT's vocabulary (see Section 4) suggest that ChatGPT does not really know French grammar and all the vocabulary, but that it works with a wide variety of tokens (graphic forms) and seems to have some difficulty with certain homographs.   To have an overview of our corpus, Table 1 provides the president names and Nnt, the number of words in each presidential message, followed by Ngpt, the number of words of the corresponding generated text", "Clearly, ChatGPT drastically avoids these difficulties.  This is reflected in the use of adverbs, the last grammatical category in the verb phrase.  Table 6 presents them according to their occurrence frequencies in presidential speeches.  From this, the first 15 are present in both lists but not in the same order.  One must also recall that this category is clearly under-use by ChatGPT, on average - 32% compared to the natural texts used for learning (see Table 2)"],
        ["chuOn the other hand, the generated speeches employ less verbs, pronouns, and adverbs and include, in mean, too standardized sentences.  Considering some words, one can observe that ChatGPT tends to overuse “to must” (devoir), “to continue” or the lemma “we” (nous).  Moreover, GPT underuses the auxiliary verb “to be” (être), or the modal verbs “to will” (vouloir) or “to have to” (falloirnk1_q3", "chuOn the other hand, the generated speeches employ less verbs, pronouns, and adverbs and include, in mean, too standardized sentences.  Considering some words, one can observe that ChatGPT tends to overuse “to must” (devoir), “to continue” or the lemma “we” (nous).  Moreover, GPT underuses the auxiliary verb “to be” (être), or the modal verbs “to will” (vouloir) or “to have to” (falloir)nk2_q3", "Clearly, ChatGPT drastically avoids these difficulties.  This is reflected in the use of adverbs, the last grammatical category in the verb phrase.  Table 6 presents them according to their occurrence frequencies in presidential speeches.  From this, the first 15 are present in both lists but not in the same order.  One must also recall that this category is clearly under-use by ChatGPT, on average - 32% compared to the natural texts used for learning (see Table 2)"],
        ["The produced messages are clear, coherent, plausible, and without spelling errors.  Facing such successes, Bubeck et al. (2023) assert that GPT “produces outputs that are essentially indistinguishable from (even better than) what humans could produce”.  The news however report also other examples of such generated texts that include both incoherence and inaccuracies (called hallucinations)", "The produced messages are clear, coherent, plausible, and without spelling errors.  Facing such successes, Bubeck et al. (2023) assert that GPT “produces outputs that are essentially indistinguishable from (even better than) what humans could produce”.  The news however report also other examples of such generated texts that include both incoherence and inaccuracies (called hallucinations)", "It is important to note that the choice of the next token is reason-able or plausible according to the training sample.  This does not mean that the produced se-quence does exist in the training documents or that the resulting statement is true.  Thus, and common to all LLMs, GPT output may include hallucinations (incorrect information) in its an-swers.  The writing assistant will just provide a reasonable or plausible next token"],
        ["In a related study, (Gao et al., 2023) indicate that human beings are less efficient to detect machine-generated passages.   More problematic is the use of such writing assistants to produce scientific papers.  Such appli-cations are not new (Labbé & Labbé, 2013) even in generating tortured phrases (Cabanac et al., 2021).  According to Gao et al. (2023), the scientific abstracts generated by GPT are hard to detect by expert in the field (success rate around 68%, high-impact journals)", "In a related study, (Gao et al., 2023) indicate that human beings are less efficient to detect machine-generated passages.   More problematic is the use of such writing assistants to produce scientific papers.  Such appli-cations are not new (Labbé & Labbé, 2013) even in generating tortured phrases (Cabanac et al., 2021).  According to Gao et al. (2023), the scientific abstracts generated by GPT are hard to detect by expert in the field (success rate around 68%, high-impact journals)", "Moreover, the specification of the sources exploited to produce the text stays unknown to the user.   When working with such writing assistants, can we discriminate between texts written by a ma-chine or a human being?  Several studies expose the effectiveness of the learning strategies able to discriminate between responses generated by GPT-3 or written by human beings (Guo et al., 2023)"],
        ["Based on trained black-box classifiers (e.g., RoBERTa), the recognition rate is rather high (around 95% to 98%).  Such effectiveness is even obtained when the target language is not English (e.g., French in (Antoun et al., 2023)).  Such a high degree could be lower when faced with a new and unknown domain or when substituting tokens by misspelled words (in such cases, the achieved accuracy rate varies from 28% to 60%", "Based on trained black-box classifiers (e.g., RoBERTa), the recognition rate is rather high (around 95% to 98%).  Such effectiveness is even obtained when the target language is not English (e.g., French in (Antoun et al., 2023)).  Such a high degree could be lower when faced with a new and unknown domain or when substituting tokens by misspelled words (in such cases, the achieved accuracy rate varies from 28% to 60%", "performance drops clearly.  In addition, such detectors must take account of updated LLMs in response to their known weaknesses.  Therefore, there is a clear requirement to acquire a style description or to derive some stylistic features associated to such LLMs and, in this study, for GPT"],
        ["As a result, more than half of these generated texts are less than 1,000 words long.  Finally, one notes that the lengths of the texts generated fluctuate considerably, suggesting that, in addition to a certain instability inherent in the method, ChatGPT draws on resources other than the texts submitted by the user.   Two questions arise", "As a result, more than half of these generated texts are less than 1,000 words long.  Finally, one notes that the lengths of the texts generated fluctuate considerably, suggesting that, in addition to a certain instability inherent in the method, ChatGPT draws on resources other than the texts submitted by the user.   Two questions arise", "sentences (from 15 to 39 words) and avoids “extraordinary” sentences namely fewer very short sentences (lengths less than 15 words) or very long ones (more than 39 words).  ChatGPT respects the mean of sentence lengths in the texts it is given as models, but randomly distributes these lengths around the mean, as evidenced by the near-equality of the three central values (mode, median, mean), a characteristic of a Gaussian distribution"],
        ["In our corpus, all NTs and GPTs5 were corrected and labeled according to the principles specified by Muller (1977).  This implies that an orthographic standardization was applied to ensure that any differences observed between NTs and GPTs do not stem from fluctuations in word spelling.  Lemmatization adds a label (lemma) to each word in the text, including its dictionary entry (for example, the infinitive of the verb) and its grammatical category", "In our corpus, all NTs and GPTs5 were corrected and labeled according to the principles specified by Muller (1977).  This implies that an orthographic standardization was applied to ensure that any differences observed between NTs and GPTs do not stem from fluctuations in word spelling.  Lemmatization adds a label (lemma) to each word in the text, including its dictionary entry (for example, the infinitive of the verb) and its grammatical category", "On the other hand, the inequality {mode < median < mean} is the main characteristic of sentence lengths in natural texts (either written or oral). Furthermore, the coefficient of relative variation (CV% in Table 10) indicates that this dispersion around the mean is lower in the corpus of generated texts than in that by the presidents.  This finding explains why, on the figure, the mode of sentences produced by ChatGPT is clearly higher than that of natural sentences"],
        ["For the verb, rare and complicated tenses and modes are avoided, such as the imperfect tense, or even forgotten altogether, like the past simple tense.  The complexity of French verb usage may explain ChatGPT relative parsimony.  The same goes for subordinate clauses, which ChatGPT seems to avoid as much as possible.  In both cases, this complexity results in multiple inflections (and therefore as many different tokens) and consequently low probabilities of occurrence", "For the verb, rare and complicated tenses and modes are avoided, such as the imperfect tense, or even forgotten altogether, like the past simple tense.  The complexity of French verb usage may explain ChatGPT relative parsimony.  The same goes for subordinate clauses, which ChatGPT seems to avoid as much as possible.  In both cases, this complexity results in multiple inflections (and therefore as many different tokens) and consequently low probabilities of occurrence", "To conclude on vocabulary, ChatGPT has difficulties to reproduce NTs for words that often appear after punctuation, with those that may belong to several grammatical categories (homographs), especially when some uses are associated with the verb and others with the noun, and finally with complex constructions such as the “verb+verb” or subordinate clauses."],
        ["The same comment is valid for other POS linked to the verbs, such as pronouns, adverbs, and complex constructions with subordinates.  On the other hand, ChatGPT shows a clear preference for common nouns, adjectives, possessives, and prepositions, probably because their associations with other words are more regular.", "The same comment is valid for other POS linked to the verbs, such as pronouns, adverbs, and complex constructions with subordinates.  On the other hand, ChatGPT shows a clear preference for common nouns, adjectives, possessives, and prepositions, probably because their associations with other words are more regular.", "On the other hand, ChatGPT tends to overuse common nouns, adjectives, possessive determiners, and prepositions.   When looking at the sentence length and its distribution, the generator seems incapable of repro-ducing the diversity of sentence lengths, a characteristic of natural texts.  Of course, these obvi-ous limitations are not insurmountable and may be mitigated in future versions of the generator"],
        ["Looking at the first row of Table 4, among presidents, the most frequently used verb is “to be” with a frequency of 31.68 per thousand words.  In texts generated by ChatGPT, it is also the first, but with a lower frequency of 22.03‰, given a difference of -30% compared with presidents as displayed in the last column.   In any natural French text, “être” (to be), “avoir” (to have) and “faire” (to do) are the three most frequently used verbs in that order", "Looking at the first row of Table 4, among presidents, the most frequently used verb is “to be” with a frequency of 31.68 per thousand words.  In texts generated by ChatGPT, it is also the first, but with a lower frequency of 22.03‰, given a difference of -30% compared with presidents as displayed in the last column.   In any natural French text, “être” (to be), “avoir” (to have) and “faire” (to do) are the three most frequently used verbs in that order", "Adverbs most frequently used by presidents compared with ChatGPT (frequencies per thousand words) In addition, “ce” (that), “le” (relative pronoun of the third person), “tout” (every), “en” (other relative pronoun), “autre” (other), are homographs of determiners or prepositions.  This means a single token for two lemmas and many possible constructions.  Moreover, these determiners and prepositions are much more frequent in French than their pronoun homographs"],
        ["One can see that the pronoun “we” is employed the most by presidents (16.1 per thousand words) and by ChatGPT (24.28‰), an increase of 51% compared to presidents.", "One can see that the pronoun “we” is employed the most by presidents (16.1 per thousand words) and by ChatGPT (24.28‰), an increase of 51% compared to presidents.", "But as mentioned earlier in the analysis of grammatical categories, in the texts generated by ChatGPT, there are 22% fewer pronouns than in the texts it is supposed to emulate.  Consequently, if ChatGPT had respected the hierarchy of presidential pronouns, the last column should contain around -22% in each row.   This aspect highlights the overuse of “we” and “you”"],
        ["This proportion should be borne in mind when interpreting the last column of Table 6.   In any French text, the negative construction (“ne+verb+pas” (verb+not)) is extremely frequent. It is almost halved in ChatGPT-generated texts, because it is employed exclusively with a verb whose subject is often a pronoun like “ce n'est pas” (it is not).  On the other hand, adverbs usually used outside the verbal group (e.g., “today”) or in a nominal group (e.g., “all”) do not suffer the", "This proportion should be borne in mind when interpreting the last column of Table 6.   In any French text, the negative construction (“ne+verb+pas” (verb+not)) is extremely frequent. It is almost halved in ChatGPT-generated texts, because it is employed exclusively with a verb whose subject is often a pronoun like “ce n'est pas” (it is not).  On the other hand, adverbs usually used outside the verbal group (e.g., “today”) or in a nominal group (e.g., “all”) do not suffer the", "For the verb, rare and complicated tenses and modes are avoided, such as the imperfect tense, or even forgotten altogether, like the past simple tense.  The complexity of French verb usage may explain ChatGPT relative parsimony.  The same goes for subordinate clauses, which ChatGPT seems to avoid as much as possible.  In both cases, this complexity results in multiple inflections (and therefore as many different tokens) and consequently low probabilities of occurrence"],
        ["Clearly, ChatGPT drastically avoids these difficulties.  This is reflected in the use of adverbs, the last grammatical category in the verb phrase.  Table 6 presents them according to their occurrence frequencies in presidential speeches.  From this, the first 15 are present in both lists but not in the same order.  One must also recall that this category is clearly under-use by ChatGPT, on average - 32% compared to the natural texts used for learning (see Table 2)", "Clearly, ChatGPT drastically avoids these difficulties.  This is reflected in the use of adverbs, the last grammatical category in the verb phrase.  Table 6 presents them according to their occurrence frequencies in presidential speeches.  From this, the first 15 are present in both lists but not in the same order.  One must also recall that this category is clearly under-use by ChatGPT, on average - 32% compared to the natural texts used for learning (see Table 2)", "same erosion or are even overused (such as “all” or “together”).  The most striking case is “également” (also), which ranks third among ChatGPT's favorite adverbs, whereas it is only 37th among presidents.  In fact, all its occurrences are found in a nominal group (generally in front of an adjective).  However, as the analysis of grammatical categories has shown, ChatGPT's favors words belonging to the noun group, such as these adverbs (the names of which should better be \"adnouns\")"],
        ["On the other hand, ChatGPT seems to have difficulty handling adjectives that are also nouns (e.g., French, European, neighbor), as well as adverbs “grand”, “bon” (big, well) or the pronoun “seul” (alone).   This problem of homographs (which is massive in French) also affects determiners as displayed in Table 9.  This table indicates that in presidential addresses, the determiner most frequently used is the article “the” with a frequency of 110.73 per thousand words.  In GPTs, it is also the", "On the other hand, ChatGPT seems to have difficulty handling adjectives that are also nouns (e.g., French, European, neighbor), as well as adverbs “grand”, “bon” (big, well) or the pronoun “seul” (alone).   This problem of homographs (which is massive in French) also affects determiners as displayed in Table 9.  This table indicates that in presidential addresses, the determiner most frequently used is the article “the” with a frequency of 110.73 per thousand words.  In GPTs, it is also the", "On the other hand, ChatGPT seems to have difficulty handling adjectives that are also nouns (e.g., French, European, neighbor), as well as adverbs “grand”, “bon” (big, well) or the pronoun “seul” (alone).   This problem of homographs (which is massive in French) also affects determiners as displayed in Table 9.  This table indicates that in presidential addresses, the determiner most frequently used is the article “the” with a frequency of 110.73 per thousand words.  In GPTs, it is also the"],
        ["The table shows that the general framework of presidential messages is indeed in the GPTs, but with a stronger emphasis on generic and recurrent theme.  For example, “wishes for the year, the future, the country or the nation” are all over-used but, strangely, the fact that these greetings are presented on the evening (December 31st) was not use by ChatGPT (e.g., “soir” (evening) –79%)", "The table shows that the general framework of presidential messages is indeed in the GPTs, but with a stronger emphasis on generic and recurrent theme.  For example, “wishes for the year, the future, the country or the nation” are all over-used but, strangely, the fact that these greetings are presented on the evening (December 31st) was not use by ChatGPT (e.g., “soir” (evening) –79%)", "When combining this table with the previous one, one can see that ChatGPT overuses the main presidential syntagms: “wishes for the new year”, “better economic and social situation"],
        ["This yields to two sets of values, summarized in Table 10.  In addition, Figure 1 shows the distribution of those lengths for both corpora.   To obtain an overview, Table 10 shows that in the corpus of presidential addresses, the most frequent length (mode) is 12 words versus 19 in the GPTs i.e., 46% longer in the generated texts.  Half of the sentences are less than 20 words long (median) in both corpora.  The average sentence length is very close (21 and 21.7 words)", "This yields to two sets of values, summarized in Table 10.  In addition, Figure 1 shows the distribution of those lengths for both corpora.   To obtain an overview, Table 10 shows that in the corpus of presidential addresses, the most frequent length (mode) is 12 words versus 19 in the GPTs i.e., 46% longer in the generated texts.  Half of the sentences are less than 20 words long (median) in both corpora.  The average sentence length is very close (21 and 21.7 words)", "Figure 1.  Comparison of sentence lengths in messages from presidents and those generated by ChatGPT (in % of total sentences in each corpus) The two distributions are very dissimilar.  The presidents (solid curve) show the typical profile seen in any natural text, namely a strong asymmetry on the left and a significant spread of lengths.  The dotted curve produced by ChatGPT falls below this solid curve at both ends of the graph, and clearly above it in the middle"],
        ["sentences (from 15 to 39 words) and avoids “extraordinary” sentences namely fewer very short sentences (lengths less than 15 words) or very long ones (more than 39 words).  ChatGPT respects the mean of sentence lengths in the texts it is given as models, but randomly distributes these lengths around the mean, as evidenced by the near-equality of the three central values (mode, median, mean), a characteristic of a Gaussian distribution", "sentences (from 15 to 39 words) and avoids “extraordinary” sentences namely fewer very short sentences (lengths less than 15 words) or very long ones (more than 39 words).  ChatGPT respects the mean of sentence lengths in the texts it is given as models, but randomly distributes these lengths around the mean, as evidenced by the near-equality of the three central values (mode, median, mean), a characteristic of a Gaussian distribution", "Figure 1.  Comparison of sentence lengths in messages from presidents and those generated by ChatGPT (in % of total sentences in each corpus) The two distributions are very dissimilar.  The presidents (solid curve) show the typical profile seen in any natural text, namely a strong asymmetry on the left and a significant spread of lengths.  The dotted curve produced by ChatGPT falls below this solid curve at both ends of the graph, and clearly above it in the middle"],
        ["Finally, in the presidential texts, the sentence length at the upper limit of the ninth decile is 3.84 times greater than that at the first decile, i.e., more than twice as long as the same interval in the GPTs.   The inequality and spread of central values of sentence lengths is a characteristic of any natural text.  To measure this, two indices have been calculated.  First one can consider the standard deviation around the mean", "Finally, in the presidential texts, the sentence length at the upper limit of the ninth decile is 3.84 times greater than that at the first decile, i.e., more than twice as long as the same interval in the GPTs.   The inequality and spread of central values of sentence lengths is a characteristic of any natural text.  To measure this, two indices have been calculated.  First one can consider the standard deviation around the mean", "On the other hand, the inequality {mode < median < mean} is the main characteristic of sentence lengths in natural texts (either written or oral). Furthermore, the coefficient of relative variation (CV% in Table 10) indicates that this dispersion around the mean is lower in the corpus of generated texts than in that by the presidents.  This finding explains why, on the figure, the mode of sentences produced by ChatGPT is clearly higher than that of natural sentences"],
        ["Finally, the absence of very long sentences in GPTs may be linked to the low frequency of complex constructions, notably subordinate clauses.   About style, a conclusion is obvious namely that ChatGPT treats punctuation like words (hence the roughly identical central values).  Therefore, the generator had identified which words are most likely to be followed by a comma, period, etc", "Finally, the absence of very long sentences in GPTs may be linked to the low frequency of complex constructions, notably subordinate clauses.   About style, a conclusion is obvious namely that ChatGPT treats punctuation like words (hence the roughly identical central values).  Therefore, the generator had identified which words are most likely to be followed by a comma, period, etc", "To conclude on vocabulary, ChatGPT has difficulties to reproduce NTs for words that often appear after punctuation, with those that may belong to several grammatical categories (homographs), especially when some uses are associated with the verb and others with the noun, and finally with complex constructions such as the “verb+verb” or subordinate clauses."],
        ["7 Can One Identify a Text Generated by ChatGPT? To date, the intertextual distance associated with automatic classifications has proved to be an effective tool for automatic recognition of the author of a text.  In particular, this method has been used to identify papers produced by previous-generation of paper mills, as well as various types of fraud in scientific publications (Byrne & Labbé, 2016), (Van Noorden, 2014), (Labbé & Labbé, 2012) or well as in authorship attribution (Savoy, 2018)", "7 Can One Identify a Text Generated by ChatGPT? To date, the intertextual distance associated with automatic classifications has proved to be an effective tool for automatic recognition of the author of a text.  In particular, this method has been used to identify papers produced by previous-generation of paper mills, as well as various types of fraud in scientific publications (Byrne & Labbé, 2016), (Van Noorden, 2014), (Labbé & Labbé, 2012) or well as in authorship attribution (Savoy, 2018)", "this conclusion only applies to the end-of-the year addresses and would need to be confirmed for all the speeches.   All this shows that ChatGPT is a good imitator.  Intertextual distance is therefore no longer able to identify texts generated by ChatGPT (as it once could be with older text generators) at least if users have taken care to submit to the generator a single homogeneous text to be “emulated”6"],
        ["In fact, the techniques used until now to detect plagiarisms or texts generated by paper mills, no longer seem appropriate.  Provided that users have taken care to submit homogeneous texts to ChatGPT, intertextual distance seems inoperative.  Similarly, conventional plagiarism detection systems are likely to be ineffective, since they analyze n-grams (or word stacks), whereas GPT rearranges the vocabulary of the model in the generated text", "In fact, the techniques used until now to detect plagiarisms or texts generated by paper mills, no longer seem appropriate.  Provided that users have taken care to submit homogeneous texts to ChatGPT, intertextual distance seems inoperative.  Similarly, conventional plagiarism detection systems are likely to be ineffective, since they analyze n-grams (or word stacks), whereas GPT rearranges the vocabulary of the model in the generated text", "7 Can One Identify a Text Generated by ChatGPT? To date, the intertextual distance associated with automatic classifications has proved to be an effective tool for automatic recognition of the author of a text.  In particular, this method has been used to identify papers produced by previous-generation of paper mills, as well as various types of fraud in scientific publications (Byrne & Labbé, 2016), (Van Noorden, 2014), (Labbé & Labbé, 2012) or well as in authorship attribution (Savoy, 2018)"],
        ["President/Year President Speeches (NTs) Nnti GPTs Ngpti   Ngpti /Nnti  Chirac    1 2002 1,026 552 0.54 2 2003 1,179 639 0.54 3 2004 1,304 446 0.34 4 2005 980 786 0.80 5 2006 1,142 420 0.37  Total Chirac 5,631 2,843 0.50  Sarkozy    6 2007 1,552 822 0.53 7 2008 1,158 770 0.66 8 2009 1,182 530 0.45 9 2010 1,216 693 0.57 10 2011 1,309 1,129 0.86  Total Sarkozy 6,417 3,944 0.61  Hollande    11 2012 1,214 1,434 1.18 12 2013 1,639 1,023 0.62 13 2014 1,547 1,213 0.78 14 2015 1,443 509 0.35 15 2016", "President/Year President Speeches (NTs) Nnti GPTs Ngpti   Ngpti /Nnti  Chirac    1 2002 1,026 552 0.54 2 2003 1,179 639 0.54 3 2004 1,304 446 0.34 4 2005 980 786 0.80 5 2006 1,142 420 0.37  Total Chirac 5,631 2,843 0.50  Sarkozy    6 2007 1,552 822 0.53 7 2008 1,158 770 0.66 8 2009 1,182 530 0.45 9 2010 1,216 693 0.57 10 2011 1,309 1,129 0.86  Total Sarkozy 6,417 3,944 0.61  Hollande    11 2012 1,214 1,434 1.18 12 2013 1,639 1,023 0.62 13 2014 1,547 1,213 0.78 14 2015 1,443 509 0.35 15 2016", "President/Year President Speeches (NTs) Nnti GPTs Ngpti   Ngpti /Nnti  Chirac    1 2002 1,026 552 0.54 2 2003 1,179 639 0.54 3 2004 1,304 446 0.34 4 2005 980 786 0.80 5 2006 1,142 420 0.37  Total Chirac 5,631 2,843 0.50  Sarkozy    6 2007 1,552 822 0.53 7 2008 1,158 770 0.66 8 2009 1,182 530 0.45 9 2010 1,216 693 0.57 10 2011 1,309 1,129 0.86  Total Sarkozy 6,417 3,944 0.61  Hollande    11 2012 1,214 1,434 1.18 12 2013 1,639 1,023 0.62 13 2014 1,547 1,213 0.78 14 2015 1,443 509 0.35 15 2016"],
        ["Figure 3. Tree classification by presidents How much confidence can one place in these classifications?  A quality index is used to answer this question (Labbé & Labbé, 2006).  For the tree (Figure 3), all the paths on the tree have an index greater than 90%, and the quality of the whole tree is greater than 96%.  In other words, there is less than a 5% chance of being wrong in saying that this tree gives the best possible representations of the information contained in the distance matrix", "Figure 3. Tree classification by presidents How much confidence can one place in these classifications?  A quality index is used to answer this question (Labbé & Labbé, 2006).  For the tree (Figure 3), all the paths on the tree have an index greater than 90%, and the quality of the whole tree is greater than 96%.  In other words, there is less than a 5% chance of being wrong in saying that this tree gives the best possible representations of the information contained in the distance matrix", "Figure 3. Tree classification by presidents How much confidence can one place in these classifications?  A quality index is used to answer this question (Labbé & Labbé, 2006).  For the tree (Figure 3), all the paths on the tree have an index greater than 90%, and the quality of the whole tree is greater than 96%.  In other words, there is less than a 5% chance of being wrong in saying that this tree gives the best possible representations of the information contained in the distance matrix"],
        ["Therefore, the target text genre is a short response within a question/answering system but the produced reply takes account of previous interactions.  According to this background, this study will focus on relatively short messages generated by ChatGPT and compared their stylistic features with true ones.   More precisely, the main aim of this study is to empirically test whether ChatGPT can adopt the style of a given author (Savoy, 2020)", "Therefore, the target text genre is a short response within a question/answering system but the produced reply takes account of previous interactions.  According to this background, this study will focus on relatively short messages generated by ChatGPT and compared their stylistic features with true ones.   More precisely, the main aim of this study is to empirically test whether ChatGPT can adopt the style of a given author (Savoy, 2020)", "However, despite these limitations, when ChatGPT is placed in the best possible conditions (a single and short model, a known author, a date, a simple genre), it manages to reproduce the main formal characteristics of a natural text that has been given to it as a model.  Moreover, the produced text makes sense to the reader who consults it.  If this reader has any suspicions, there is currently no computerized tool to help confirm them"],
    ],
    "ground_truth": [
        "The study compared addresses written by Chirac, Sarkozy, Hollande, and Macron",
        "ChatGPT tends to overuse nouns, possessive determiners, and numbers.",
        "ChatGPT tends to overuse the verbs 'to must' (devoir) and 'to continue', as well as the lemma 'we' (nous).",
        "Hallucinations are defined as incorrect information, incoherence, and inaccuracies included in generated answers.",
        "The recognition rate is rather high, around 95% to 98%.",
        "The accuracy rate varies from 28% to 60% when faced with a new and unknown domain or when substituting tokens by misspelled words",
        "On average, the generated texts are almost half as long as the models.",
        "It was applied to ensure that any differences observed between natural texts and generated texts do not stem from fluctuations in word spelling.",
        "ChatGPT avoids rare and complicated tenses, such as the imperfect tense, or forgets them altogether, like the past simple tense.",
        "ChatGPT favors them probably because their associations with other words are more regular.",
        "The three most frequently used verbs are 'être' (to be), 'avoir' (to have) and 'faire' (to do) in that order.",
        "ChatGPT used the pronoun 'we' 24.28 times per thousand words, representing an increase of 51% compared to presidents.",
        "It is almost halved because it is employed exclusively with a verb whose subject is often a pronoun like 'ce n'est pas' (it is not).",
        "The adverb 'également' (also) ranks third among ChatGPT's favorite adverbs, whereas it is only 37th among presidents.",
        "The noun 'défi' (challenge) saw a 696% increase. This is because ChatGPT frequently repeats the stereotyped formula 'relever les défis qui' (take up the challenge which).",
        "ChatGPT corrects the presidents by stating the feminine form before the masculine one: 'nos concitoyennes et nos concitoyens' (our fellow citizen women and our fellow citizen men).",
        "The text states the most frequent length (mode) is 12 words for presidents versus 19 in the GPTs. (Note: Table 10 lists the presidential mode as 13, representing a minor internal typo in the paper itself).",
        "ChatGPT over-employs 'verage' sentences (from 15 to 39 words) and avoids 'extraordinary' sentences (very short or very long ones). The generated sentence lengths are significantly less diverse or spread out than in natural texts.",
        "The inequality $mode<median<mean$ is the main characteristic of sentence lengths in natural texts",
        "ChatGPT treats punctuation like words, having identified which words are most likely to be followed by a comma, period, etc..",
        "Intertextual distance is no longer able to identify texts generated by ChatGPT, at least if users have taken care to submit to the generator a single homogeneous text to be emulated.",
        "Conventional plagiarism detection systems are likely to be ineffective since they analyze n-grams (or word stacks), whereas GPT rearranges the vocabulary of the model in the generated text.",
        "Chirac's messages are quite clearly out of step with those of his three successors.",
        "The quality of the whole tree is greater than 96%",
        "The target text genre is a short response within a question/answering system."

    ]
}


dataset = Dataset.from_dict(data)

results = evaluate(
    dataset=dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ],
    llm=evaluator_llm
)

print(results)
