STEP 1 â€” core/config.py (START HERE)

Why first?
Everything else depends on config. Changing this later causes refactors.

File: app/core/config.py

What to define:

model names

chunk size

overlap

Qdrant URL

collection name

Example (minimal):

import os

EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4.1-mini"

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
COLLECTION_NAME = "documents"


Stop here and commit.

STEP 2 â€” Ingestion (MOST IMPORTANT)

If ingestion is wrong, everything else is garbage.

2A. ingestion/loader.py

Goal: Load raw text, preserve pages.

Implement:

PDF loader

TXT loader

No chunking. No embeddings.

2B. ingestion/chunker.py

Goal: Deterministic chunking + metadata.

Implement:

chunk size

overlap

attach doc_id, page, source

This file should be pure logic.

2C. ingestion/embedder.py

Goal: Convert text â†’ vectors.

Implement:

embed_texts()

embed_query()

No database. No FastAPI.

ğŸ” Verification checkpoint (do NOT skip)

Before moving on:

Same PDF â†’ same number of chunks every run

Every chunk has metadata

No OpenAI call outside embedder.py

If this fails â†’ stop.

STEP 3 â€” Vector Store Layer
3A. retrieval/qdrant.py

Goal: Encapsulate Qdrant.

Implement:

create/init collection

upsert points

raw search

No thresholds yet. No filtering logic.

3B. retrieval/retriever.py

Goal: Retrieval logic.

Implement:

top-k

score threshold

metadata filters

This is where â€œretrieval techniquesâ€ live.

ğŸ” Verification checkpoint

Query returns chunks

Filters work (doc_id)

Low-score chunks are excluded

Only after this is retrieval â€œdoneâ€.

STEP 4 â€” Generation (ONLY NOW)
4A. generation/prompts.py

Goal: Write prompts as plain text.

Include:

citation rules

refusal rules (â€œif context insufficientâ€¦â€)

No logic. Just strings.

4B. generation/generator.py

Goal: LLM call + answer assembly.

Input:

query

retrieved chunks

Output:

answer

cited chunk IDs

Do NOT do retrieval here.

STEP 5 â€” API Layer (FastAPI)

Only now do you touch FastAPI.

5A. api/ingest.py

Calls:

loader â†’ chunker â†’ embedder â†’ qdrant.upsert

5B. api/query.py

Calls:

embed_query â†’ retriever â†’ generator


Routes should be thin (10â€“20 lines max).

STEP 6 â€” Evaluation (After system works)
evaluation/ragas_eval.py

Implement:

load eval dataset

call /query

compute RAGAS metrics

store results

If you do this earlier, youâ€™ll waste time.

STEP 7 â€” Observability (Last, but valuable)

Add:

latency logging

token counting

rough cost estimation

This turns â€œworksâ€ into â€œproduction-awareâ€.

Correct Mental Model (tattoo this)
Layer	Determinism
Config	100%
Ingestion	95%
Retrieval	85%
Generation	40%
Evaluation	Meta

Always code from top determinism â†’ bottom determinism.

Common wrong starting points (avoid these)

âŒ Starting with FastAPI routes
âŒ Writing prompts before retrieval
âŒ Adding RAGAS before ingestion works
âŒ Writing everything in one file â€œtemporarilyâ€

There is no â€œtemporaryâ€ in production code.